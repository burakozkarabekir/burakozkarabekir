# -*- coding: utf-8 -*-
"""Group18-Progress Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GP9rklfFNJ4ivnbgKhGrrmaNQ6I4utxC

# [Covid-19 and Character Strengths- Group 18]

Group Members:
Ayşenur Güller,
Emre Coşkuner,
Berkin Divrikli,
Halil Serbey Celep,
Burak Özkarabekir.

## Introduction

<font color="black">
The COVID-19 pandemic affected people by creating negative psychological consequences due to self-isolation terms, health worries and lockdowns. The data that collected from 944 people that have different characteristics strengths. This project focuses on reflecting the protective role of character strengths, lockdown related self-efficacy by analyzing the given dataset. The project, especially will be focusing on DASS which is Depression Anxiety and Stress Scale, try to determine the strength factors which increases those values. Strength factors are the ones which constitutes DASS values. Openness level, which is the level of a person’s imagination, curiosity and open mindedness. Restraint is control over thought or feelings. Transcendence is the most complete version of a human. Interpersonal is relating communication or relationship of people.
</font>

### Problem Definition

<font color="black">
First and foremost, rather than specifying a problem and attempting to find a possible null solution to the problem we created, it would be more accurate to identify this project as the most possible heuristic approach to exploring the main dataset(DB), constructing a better understanding of the entire concept that we are dealing with, and coming up with profound and effective predictions and foundations. 

Furthermore, we acted in a more experimenting and investigating manner. To generalize, we want to extract details from our data and support the details with intriguing outcomes from our supplement datasets in order to identify hidden relations and diverse aspects buried in the dataset.
</font>


We therefore hypothesize that transcendence and restraint second-order strengths may relate to lower levels of psychological distress (i.e. depression, anxiety and stress), and higher levels of Covid-19-related self-efficacy.

### Utilized Datasets

<font color="black">

Data refers to an analysis of the protective role of character strengths on personal stress and anxiety levels during pandemic period. The dataset contains participants numbered, strengths factors which are "Openness, Restrain, Transcendence, Interpersonal", constant measures which are DASS21, GHQ12, SEC and personality informations that are age, gender, work, being a student and days passed when the responses collected from participants and lastly, son number. After these datasets, characteristic strengths such as creativity, hope, leadership, love are recorded in the given excel file. However, in our project, we will only use strength 4 factors which are Openness, Restrain, Transcendence, Interpersonal to find correlation between these strength factors and psychological distress and self-efficacy.

The data is accessed from an excel file which can be found from the link: https://www.kaggle.com/mathchi/covid19-and-character-strengths
</font>

**1.Adding libraries and mounting google drive process**
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("./drive")

path_prefix = "./drive/My Drive"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
import numpy as np



# %matplotlib inline

import folium
from folium import plugins
from folium.plugins import HeatMap
import seaborn as sns


import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from scipy import stats

"""**2.Adding files to the notebook**

"""

#In order to open DB excel
df = pd.read_excel('DB.xlsx')

#In order to make sure not to make any change on actual data
df_c = df.copy()

"""**a. Data detailed overview**"""

df_c.shape

"""*Columns are as follows*"""

df_c.columns

"""*Datatypes are as follows*"""

df_c.dtypes

"""General Look and some changes in the data for the data exploration"""

df_c.head()

df_c.isnull().sum()

"""As we can see clearly, there is different data types as int64 and object. So make our data more usable, we will change our object types to int64"""

df_c.replace(to_replace={'Student': 1, 'Other': 2}, inplace=True)
df_c.replace(to_replace={'Male': 0, 'Female': 1}, inplace=True)
df_c.fillna(0,inplace=True)
df_c[['Student','Gender']]= df_c[['Student','Gender']].astype('int64')

df_c['Student'].dtypes

df_c['Gender'].dtypes

"""NOW our data, filled and converted:


Student categarization --> 'Nan' : 0 , 'Student': 1, 'Other' : 2


Gender categorization --> 'Male' : 0 , 'Female' :1

## Data Exploration

<font color="blue">



</font>
"""

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

plt.figure(figsize=(16,12))

heatmap = sns.heatmap(df.corr(), vmin=-1,vmax=1, annot=False, cmap='viridis')

heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)
plt.show()

"""We can observe correlation by using heatmap above; however, the second correlation matrix below makes more visible to correlation values.


We know that if correlation value close to -1.0 and 1.0, it means that this features should be considered as correlated.



"""

df_c.corr().style.background_gradient(cmap = 'coolwarm')

"""SOME OBSERVATIONS FROM ABOVE


As we can see on the correlation matrix, **DASS_21**, **GHQ_12** and **SEC** mostly correlated with

DASS_anxiety
DASS_depression
DASS_stress





Thus, in order to understand which features mostly influence DASS scores of the given datasets we should drop them from the dataset. 



We choose our targets as DASS_anxiety, DASS_depression ,DASS_stress

"""

df_c= df_c.drop(['DASS_21', 'GHQ_12', 'SEC'],axis=1)

"""Since we do not need to observe participation numbers, we will drop this column

"""

df_c = df_c.drop(['Participant'], axis=1)

"""Also, it can be observed that 'Openness, Restrain, Transcedence, Interpersonal' affects our target on the same correlation. Therefore, we can drop these features to make more clear observations in our data.

"""

df_c =df_c.drop(['Openness','Restraint','Transcendence',
           'Interpersonal'],axis=1)

features = df_c.iloc[:,3:-3]

features.columns.values.tolist()

"""## Machine Learning Models

<font color="black">
We will use classification and regression machine learning models to analyze and inspect our work. By using regression algorithms, we will predict values according to our data and our hypothesis. Likewise, classification algorithm will help us to divide the participants and classify. The rest work will be shown in our final project.
</font>

**TESING AND COMPARING DIFFERENT CLASSIFICATION MODEL**
"""

df_c.DASS_anxiety.describe()

X = df_c.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)
y = df_c['DASS_anxiety']
print('Anxiety Distribution:')
print(y.value_counts())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix as cm

X.head()

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)


rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=42)
rf.fit(X_train, y_train)
pred_rf = rf.predict(X_test)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix as cm

"""*Accurarcy*: Accuracy is the value that the machine learning algorithm calculated by dividing true positive and true negative values over all observations. Having accuracy value close to 1 means that the machine learning algorithm that created having good predictions over observations. """

from sklearn import metrics
max_acc=-1
index=-1
for i in range(1,40):
  rf  = RandomForestClassifier(max_depth=i)
  
  rf.fit(X_train, y_train)
  pred_rf = rf.predict(X_test)
  acc=metrics.accuracy_score(y_test, pred_rf)
  print("Accuracy of the random forest model: ",acc,"best at depth",i)
  if max_acc < acc:
    max_acc=acc
    index=i
  
print("Accuracy of the random forest model: ",max_acc,"best at depth",index)

rf=RandomForestClassifier(max_depth=index)
rf.fit(X_train, y_train)
pred_rf = rf.predict(X_test)
rf.feature_importances_

predictions = rf.predict(X_test)


score = round(accuracy_score(y_test, predictions), 3)
cm1 = cm(y_test, predictions)
sns.heatmap(cm1, annot=True, fmt=".0f")
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Accuracy Score: {0}'.format(score), size = 5)
plt.show()

from sklearn.metrics import classification_report

print(classification_report(y_test, predictions))

rf = RandomForestClassifier(n_estimators=100, max_depth = 8, random_state=42)
rf.fit(X_train, y_train)
predictions = rf.predict(X_test)

print(classification_report(y_test, predictions))
plt.figure(figsize=(16, 9))

ranking = rf.feature_importances_
features = np.argsort(ranking)[::-1][:10]
columns = X.columns

plt.title("Feature importances based on Random Forest Classifier", y = 1.03, size = 18)
plt.bar(range(len(features)), ranking[features], color="Red", align="center")
plt.xticks(range(len(features)), columns[features], rotation=80)
plt.show()

feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(12, 12))
sns.barplot(x=feature_importances, y=feature_importances.index)

# Add labels to our graph  
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Feature Importance Rankings")
plt.show()

"""Since the accuracy score that we get here is not close to 1, it is observed that our machine learning algorithm is not succesful in making predictions over observations. To have better accuracy, we need to change our features to observe DASS_levels."""

df_new = df.copy()
df_corr = df_new.drop(["Participant","DASS_21", "GHQ_12","SEC","Age","Work","Day","Sons",
                    "Appreciation_of_beauty","Bravery","Creativity","Curiosity","Fairness",
                    "Forgiveness","Gratitude","Honesty","Hope", "Humilty","Humor","Judgment",
                    "Kindness","Leadership","Love","Love_of_learning","Perseverance","Perspective",
                 "Prudence","Self_regulation","Social_intelligence","Spirituality","Teamwork","Zest"],axis=1).corr()

df_corr_vals = df_corr.values

fig = plt.figure()
plt.imshow(df_corr_vals,cmap="Blues")

plt.yticks(range(len(df_corr.columns)),df_corr.columns)
plt.xticks(range(len(df_corr.columns)),df_corr.columns,rotation=90)

plt.colorbar()
plt.show()

df_c2 = df.copy()
df_c2= df_c2.drop(['Gratitude',"Participant", 'Prudence', 
                   'Self_regulation', 'Fairness', 'Hope', 'Zest', 'Perseverance',
               'Social_intelligence', 'Leadership', 'Love_of_learning', 'Teamwork', 
               'Curiosity', 'Humilty', 'Creativity', 'Day', 'Judgment', 'Work', 'Sons', 'Student', 'Gender'],axis=1)
df_c2.head()

X = df_c2.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)
y = df_c2['DASS_anxiety']


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)


rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=42)
rf.fit(X_train, y_train)
pred_rf = rf.predict(X_test)

from sklearn import metrics
max_acc=-1
index=-1
for i in range(1,40):
  rf  = RandomForestClassifier(max_depth=i)
  
  rf.fit(X_train, y_train)
  pred_rf = rf.predict(X_test)
  acc=metrics.accuracy_score(y_test, pred_rf)
  print("Accuracy of the random forest model: ",acc,"best at depth",i)
  if max_acc < acc:
    max_acc=acc
    index=i
  
print("Accuracy of the random forest model: ",max_acc,"best at depth",index)

"""By changing our features, we increased our accuracy level to 0.34. However, despite the increase in accuracy score, it is not in the level that we want to observe. As mentioned above, it is important to have accuracy score close to 1 to get consistent predictions. Therefore, we will drop the features that decrease the accuracy score.

DECISION TREE CLASSIFICATION
"""

from sklearn import tree
from sklearn.model_selection import cross_val_score


X = df_c2.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)
y = df_c2['DASS_anxiety']


X_train, X_test,Y_train, Y_test= train_test_split(X, y, test_size=0.20, random_state= 10)

for i in range(1,20):

  decision_model= tree.DecisionTreeClassifier(min_samples_leaf= (((i-1)% 5) + 1), max_depth=i)
  decision_model.fit(X_train,Y_train)
  pred_decision_Y= decision_model.predict(X_test)

  print("Decision tree model with min_samples_leaf ", (((i-1)% 5) + 1)," at max_depth ", i, " accuracy score is equal to: ", accuracy_score(Y_test, pred_decision_Y))

"""NAIVE BAYESIAN CLASSIFICATION"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.naive_bayes import GaussianNB

X = df_c2.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)
Y = df_c2['DASS_anxiety']


X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
gaussian_naive_bayes_2 = GaussianNB()
y_pred = gaussian_naive_bayes_2.fit(X_train, Y_train).predict(X_test)

print("Number of mislabeled points out of a total %d points : %d"
#    % (X_test.shape[0], (Y_test != y_pred).sum()))

print("Accuracy:", accuracy_score(Y_test, y_pred))

"""DASS ANXIETY vs AGE

From the plot below we detect the correlation between age and the DASS_anxity level. It has been observed that younger participants may be more likely to stress.
"""

from sklearn.datasets import make_blobs

X = df['Age'].values

Y = df['DASS_anxiety'].values

plt.scatter(X,Y, s=50, cmap='rainbow');

"""DASS ANXIETY vs GENDER"""

from sklearn.datasets import make_blobs

X = df['Gender'].values

Y = df['DASS_anxiety'].values

plt.ylim([0.0, 25.0])

plt.xticks([0,1])


plt.scatter(X,Y, s=40, cmap='rainbow');

"""Although there is little difference, when the susceptibility of women and men to stress is examined, it has been observed that women are slightly more likely to get stressed than men.

**TESTING AND COMPARING DIFFERENT REGRESSION MODELS**
"""

from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

"""RANDOM FOREST REGRESSION"""

df_c = df.copy()
df_c.replace(to_replace={'Student': 1, 'Other': 2}, inplace=True)
df_c.replace(to_replace={'Male': 0, 'Female': 1}, inplace=True)
df_c.fillna(0,inplace=True)
df_c[['Student','Gender']]= df_c[['Student','Gender']].astype('int64')
df_c= df_c.drop(['DASS_21', 'GHQ_12', 'SEC'],axis=1)
df_c = df_c.drop(['Participant'], axis=1)
df_c =df_c.drop(['Openness','Restraint','Transcendence',
           'Interpersonal'],axis=1)
features = df_c.iloc[:,3:-3]

from sklearn.ensemble import RandomForestRegressor
Y = df_c['DASS_anxiety']
X = df_c.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20, random_state = 1)

regression_model2 = RandomForestRegressor( n_estimators= 100  )
regression_model2.fit(X_train, Y_train)
pred2_Y= regression_model2.predict(X_test)
print("Mean absolute error of test is equal to", mean_absolute_error(Y_test, pred2_Y))
pred2_Y_train = regression_model2.predict(X_train)
print("Mean absolute error of train is equal to", mean_absolute_error(Y_train, pred2_Y_train))

mse = mean_squared_error(Y_test, pred2_Y)
mae = mean_absolute_error(Y_test, pred2_Y)
rmse = np.sqrt(mse)

print("\nFinal results:\n")
print("mean squared error of test: {}".format(mse))
print("mean absolute error: {}".format(mae))
print("root of mean squared error: {}".format(rmse))

from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error, mean_absolute_error

mae = mean_absolute_error(Y_test, pred2_Y)

xx = range(0,len(X_test))
fig, ax = plt.subplots(sharex= True, figsize=(16,9))
fig.suptitle('Random Forest Regressor model predictions')

ax.plot(xx, sorted(Y_test), color='b', label='Anxiety Actual')
ax.plot(xx, sorted(pred2_Y), color='k', label='Anxiety Predicted', alpha=0.7)
ax.fill_between(xx, sorted(pred2_Y)+mae, sorted(pred2_Y)-mae,
                     color = 'k',alpha=0.15)
ax.legend()
ax.set_title('Anxiety Scale\n MAE: {:.2f}'.format(mae))

plt.show()

"""*K-NEAREST NEIGHBORS REGRESSION*"""

from sklearn.neighbors import KNeighborsRegressor

Y = df_c['DASS_anxiety']
X = df_c.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)

regression_model3 = KNeighborsRegressor( )
regression_model3.fit(X_train,Y_train)
pred3_Y= regression_model3.predict(X_test)
pred3_Y_train = regression_model3.predict(X_train)
print("Mean squared error of test is equal to", mean_squared_error(Y_test,pred3_Y))
print("Mean squared error of train is equal to", mean_squared_error(Y_train,pred3_Y_train))

mse = mean_squared_error(Y_test, pred3_Y)
mae = mean_absolute_error(Y_test, pred3_Y)
rmse = np.sqrt(mse)

print("\nFinal results:\n")
print("mean squared error of test: {}".format(mse))
print("mean absolute error: {}".format(mae))
print("root of mean squared error: {}".format(rmse))

xx = range(0,len(X_test))
fig, ax = plt.subplots(sharex= True, figsize=(16,9))
fig.suptitle('KNeighbors Regressor model predictions')

ax.plot(xx, sorted(Y_test), color='b', label='Anxiety Actual')
ax.plot(xx, sorted(pred3_Y), color='k', label='Anxiety Predicted', alpha=0.7)
ax.fill_between(xx, sorted(pred3_Y)+mae, sorted(pred3_Y)-mae,
                     color = 'k',alpha=0.15)
ax.legend()
ax.set_title('Anxiety Scale\n MAE: {:.2f}'.format(mae))

plt.show()

"""OVERALL COMPARISION"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import VotingRegressor

Y = df_c['DASS_anxiety']
X = df_c.drop(['DASS_anxiety', 'DASS_stress','DASS_depression'], axis=1)

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)



reg1 = RandomForestRegressor(n_estimators = 100, max_features =  4, random_state=42)
reg2 = LinearRegression()
reg3=KNeighborsRegressor()


reg1.fit(X_train, Y_train)
reg2.fit(X_train, Y_train)
reg3.fit(X_train, Y_train)


ereg = VotingRegressor([ ("rf", reg1), ("kn", reg3)])
ereg.fit(X_train, Y_train)

pred1 = reg1.predict(X_test)
pred2 = reg2.predict(X_test)
pred3=reg3.predict(X_test)
pred4 = ereg.predict(X_test)

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

print("Pred1: mse" , mean_squared_error(Y_test, pred1) , "r2", r2_score(Y_test, pred1) )
print("Pred2: mse" , mean_squared_error(Y_test, pred2) , "r2", r2_score(Y_test, pred2) )
print("Pred3: mse" , mean_squared_error(Y_test, pred3) , "r2", r2_score(Y_test, pred3) )
print("Pred4: mse" , mean_squared_error(Y_test, pred4) , "r2", r2_score(Y_test, pred4) )

"""**EVALUATION**

CLASSIFICATION ALGORITHMS

We worked on 3 different classification algorithms using DASS levels; Naive Baesian Classification, Decision Tree Classification and Random Forest Classification. Together with the Decision Tree Classification, we examined the effect of the DASS level on the 4 characteristic factors we selected and saw that our accuracy value was low. Finally, using Random Forest Classification, we found that DASS levels were most dependent on openness, trenscedelence, restraint, interpersonal, age, perspective, and spirituality. In addition, Random Forest Classification gave us the most appropriate precision, making it the most logical choice among Classification methods.
The best Machine Learning Algorithm above classification methods is Random Forest Classification model that has the highest accuracy rate with best prediction consistency from the other methods.






REGRESSION ALGORITHMS


Mean Squared Error is the average squared difference between the estimated values and the actual value.
As it can be seen in the above, we have calculated regression results according to different machine learning models. As a result, MSE of the Random Forest Regression is less than K-Nearest Neighbours Regression. Which means that the error between estimated and actual value is less. Due to this reason, Random Forest should be chosen in order to get the best result.

### Results & Discussion

<font color="black">
According to our heatmap, SEC and DASS_21 keys are increasing the DASS Anxiety, Stress and Depression levels significantly. In our final work wee will use machine learning algorithms and try to find the reason and solution for this correlation. 
</font>

In this project, we investigated the relation of mental health and behaviours on lockdown during COVID-19 Period.We invetigated our data according to DASS21 values which is (Depression Anxiety and Stress Scale). During the Pandemic people from different age and gender have been affected differently. Furthermore , DASS values are the best way of examining those effects.




In conclusion, we have observed that there are two features that affect DASS levels which are personality strengths such as openess and characteristic features such as gender, age DASS_anxiety, DASS_stress, DASS_depression levels are depended on these features and changes by correlation over these variables.



All characteristic features and personality strenghts has impact on DASS_levels during the COVID19 pandemic period. Some personalities and characteristics decrease those DASS_levels strongly while others may have stronger/weaker impact on DASS_levels.

Personality strengths have more weight on DASS_levels because when these datas dropped from observation, it is observed that our machine learning algorithm has lower accuracy seen in the RandomForestClassifier methods.Therefore, the personality strengths seen in data must be considered during the study when the algorithm works on data.

DASS_21, SEC AND GHQ datas are the most correlated datas with DASS_levels that can observed from correlation matrix.


In the Bayesian Classification, we examined the effects of people's age and gender on DASS levels. Younger people were more likely to have DASS levels affected than older people. We examined the effect of gender on DASS levels with Bayesian Classification, we observed that there was an effect of gender again, but it was not as big as the effect of age.
"""